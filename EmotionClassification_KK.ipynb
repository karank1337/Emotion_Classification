{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99be71f3-5252-4bdb-b5f5-3afad5924830",
   "metadata": {},
   "source": [
    "# Emotion Classification by Karan Deepak Kapadia\n",
    "\n",
    "This notebook implements an emotion classification system using a deep learning model built **entirely from scratch**.  \n",
    "The dataset (`Emotion-Dataset.csv`) consists of **text samples labeled with emotions**.\n",
    "\n",
    "### **Key Features**\n",
    "- Text preprocessing (cleaning, tokenization, encoding)  \n",
    "- Custom deep learning model (**LSTM-based classifier**)  \n",
    "- Training with optimized hyperparameters  \n",
    "- Performance evaluation and visualization  \n",
    "\n",
    "Each **code cell** is preceded by a **text explanation**, as required by the submission guidelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f259d5ab-ffd2-49ac-b4c3-c5b64151fdce",
   "metadata": {},
   "source": [
    "## 1. Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45320ab-478d-473f-a36d-4dcb83192992",
   "metadata": {},
   "source": [
    "### Setup & Imports\n",
    "\n",
    "Import necessary libraries. I also set the random seed for reproducibility and configure the device to use the GPU if available. This ensures that the experiments are consistent and run efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "036652ea-1f61-461a-b4c4-02ba141e6032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# Random seeds for reproducibility.\n",
    "seed = 420\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916ddf40-f0c9-4430-b4d0-f5b52e445563",
   "metadata": {},
   "source": [
    "## Load and Inspect Dataset\n",
    "\n",
    "The CSV file (`Emotion-Dataset.csv`) is loaded, keeping only relevant columns (`Text` and `Emotion`).  \n",
    "This step also checks for missing values and dataset shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5679ea0-8a05-45ee-a628-6de8efe7eea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text   Emotion  Unnamed: 2  \\\n",
      "0  i could get depressed about feeling isolated b...   Sadness         NaN   \n",
      "1  i am so thankful that though things are a bit ...      Fear         NaN   \n",
      "2  i remember one day years ago when the kids wer...     Anger         NaN   \n",
      "3  i feel so funny he have no topic to chat with ...  Surprise         NaN   \n",
      "4  i didnt take it personally but i could feel so...      Fear         NaN   \n",
      "\n",
      "   Unnamed: 3  Unnamed: 4  Unnamed: 5  Unnamed: 6  \n",
      "0         NaN         NaN         NaN         NaN  \n",
      "1         NaN         NaN         NaN         NaN  \n",
      "2         NaN         NaN         NaN         NaN  \n",
      "3         NaN         NaN         NaN         NaN  \n",
      "4         NaN         NaN         NaN         NaN  \n",
      "Column names: Index(['Text', 'Emotion', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4',\n",
      "       'Unnamed: 5', 'Unnamed: 6'],\n",
      "      dtype='object')\n",
      "Missing values:\n",
      " Text       0\n",
      "Emotion    0\n",
      "dtype: int64\n",
      "Dataset shape: (150000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV \n",
    "df = pd.read_csv(\"Emotion-Dataset.csv\")\n",
    "\n",
    "# Display the first few rows and column names.\n",
    "print(df.head())\n",
    "print(\"Column names:\", df.columns)\n",
    "\n",
    "# Keep only the essential columns.\n",
    "df = df[['Text', 'Emotion']]\n",
    "\n",
    "# Remove rows with missing values.\n",
    "df.dropna(inplace=True)\n",
    "print(\"Missing values:\\n\", df.isnull().sum())\n",
    "print(f\"Dataset shape: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e546dd7c-7756-430e-809c-fe02483f677f",
   "metadata": {},
   "source": [
    "### Normalize & Encode Emotion Labels\n",
    "\n",
    "The emotion labels are **converted to lowercase** to ensure consistency (e.g., merging `\"Fear\"` and `\"fear\"`).  \n",
    "Then, each label is assigned a unique numeric ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2898cb8f-11b5-436c-bf35-28bee1acc294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Emotion Mapping: {'anger': 0, 'fear': 1, 'joy': 2, 'sadness': 3, 'surprise': 4}\n"
     ]
    }
   ],
   "source": [
    "# Normalize by converting all emotion labels to lowercase.\n",
    "df['Emotion'] = df['Emotion'].str.lower()\n",
    "\n",
    "# Define the mapping (after normalization, only one version of fear remains).\n",
    "emotion_mapping = {\n",
    "    'anger': 0,\n",
    "    'fear': 1,\n",
    "    'joy': 2,\n",
    "    'sadness': 3,\n",
    "    'surprise': 4\n",
    "}\n",
    "\n",
    "df['emotion_label'] = df['Emotion'].map(emotion_mapping)\n",
    "print(\"Final Emotion Mapping:\", emotion_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207c6481-3bac-46fc-8ba1-c66886027b6f",
   "metadata": {},
   "source": [
    "### Split Data into Training & Testing Sets\n",
    "\n",
    "An **80/20 split** ensures a **balanced dataset**, maintaining similar distributions across training/testing samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cabea848-38f3-485a-b6a1-40ded41b6829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 120000\n",
      "Test samples: 30000\n"
     ]
    }
   ],
   "source": [
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['Text'], df['emotion_label'], test_size=0.2, random_state=seed, stratify=df['emotion_label']\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_texts)}\")\n",
    "print(f\"Test samples: {len(test_texts)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565c9aec-6063-441e-9ccb-5ec177ca27cf",
   "metadata": {},
   "source": [
    "### Build Vocabulary & Encode Text\n",
    "\n",
    "The model needs numerical inputs, so the **training data is tokenized** and converted into **integer sequences**.  \n",
    "A vocabulary is built from the training samples, and sentences are **padded** to a fixed length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6293213-d262-4b43-83e1-44710baa27e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 49548\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "def build_vocab(texts, min_freq=1):\n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        tokens = tokenize(text)\n",
    "        counter.update(tokens)\n",
    "    # Reserve indices for <PAD> and <UNK>\n",
    "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    for word, freq in counter.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[word] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "# Build vocabulary from training texts\n",
    "vocab = build_vocab(train_texts, min_freq=1)\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "\n",
    "def encode_text(text, vocab):\n",
    "    tokens = tokenize(text)\n",
    "    return [vocab.get(token, vocab[\"<UNK>\"]) for token in tokens]\n",
    "\n",
    "# Set a fixed maximum sequence length.\n",
    "max_seq_len = 50\n",
    "\n",
    "def pad_sequence(seq, max_length):\n",
    "    if len(seq) < max_length:\n",
    "        return seq + [0] * (max_length - len(seq))\n",
    "    return seq[:max_length]\n",
    "\n",
    "# Encode and pad the training and test texts.\n",
    "encoded_train = [pad_sequence(encode_text(text, vocab), max_seq_len) for text in train_texts]\n",
    "encoded_test = [pad_sequence(encode_text(text, vocab), max_seq_len) for text in test_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b1e653-f216-425e-9b6b-67b88975159f",
   "metadata": {},
   "source": [
    "### Create PyTorch Dataset & DataLoaders\n",
    "\n",
    "A **PyTorch Dataset** serves the encoded sequences & labels.  \n",
    "DataLoaders allow efficient **batch processing** during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6073659a-b23f-4f64-bb60-8d0d64396ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encoded_texts, labels):\n",
    "        self.encoded_texts = encoded_texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.encoded_texts[idx], dtype=torch.long), torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
    "\n",
    "# Reset indices for labels to maintain alignment.\n",
    "train_labels = train_labels.reset_index(drop=True)\n",
    "test_labels = test_labels.reset_index(drop=True)\n",
    "\n",
    "train_dataset = TextDataset(encoded_train, train_labels)\n",
    "test_dataset = TextDataset(encoded_test, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bd63c0-768b-4507-a888-2d0fb553fe09",
   "metadata": {},
   "source": [
    "### Baseline Model\n",
    "We first define a simple LSTM-based classifier. This baseline model consists of an Embedding layer, a single unidirectional LSTM layer, and a fully connected (Linear) layer. No additional regularization (such as dropout) is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "430096f6-0e7f-4f73-a5b3-f94ae85213ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMEmotionClassifier(\n",
      "  (embedding): Embedding(49548, 128)\n",
      "  (lstm): LSTM(128, 256, batch_first=True)\n",
      "  (fc): Linear(in_features=256, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LSTMEmotionClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n",
    "        super(LSTMEmotionClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)  \n",
    "        lstm_out, _ = self.lstm(embeds)  \n",
    "        last_output = lstm_out[:, -1, :]  \n",
    "        logits = self.fc(last_output)\n",
    "        return logits\n",
    "\n",
    "# Hyperparameters for baseline model.\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "vocab_size = len(vocab)\n",
    "num_classes = len(emotion_mapping)\n",
    "\n",
    "baseline_model = LSTMEmotionClassifier(vocab_size, embedding_dim, hidden_dim, num_classes).to(device)\n",
    "print(baseline_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825bd2f6-cdcb-43ad-bb0a-e8bd2ebdeb2f",
   "metadata": {},
   "source": [
    "## Train Baseline Model\n",
    "The baseline model is trained using the Adam optimizer (with weight decay) and cross-entropy loss for 10 epochs. Training loss and accuracy are displayed in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07fe4c68-b120-4751-ae50-b27ae0c67139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Baseline Model:\n",
      "Epoch 1/20 - Loss: 1.492, Accuracy: 31.76%\n",
      "Epoch 2/20 - Loss: 1.407, Accuracy: 38.29%\n",
      "Epoch 3/20 - Loss: 1.315, Accuracy: 43.55%\n",
      "Epoch 4/20 - Loss: 1.308, Accuracy: 43.84%\n",
      "Epoch 5/20 - Loss: 1.306, Accuracy: 43.96%\n",
      "Epoch 6/20 - Loss: 1.304, Accuracy: 44.01%\n",
      "Epoch 7/20 - Loss: 1.303, Accuracy: 44.05%\n",
      "Epoch 8/20 - Loss: 1.302, Accuracy: 44.05%\n",
      "Epoch 9/20 - Loss: 1.301, Accuracy: 44.09%\n",
      "Epoch 10/20 - Loss: 1.300, Accuracy: 44.13%\n",
      "Epoch 11/20 - Loss: 1.300, Accuracy: 44.16%\n",
      "Epoch 12/20 - Loss: 1.299, Accuracy: 44.11%\n",
      "Epoch 13/20 - Loss: 1.299, Accuracy: 44.12%\n",
      "Epoch 14/20 - Loss: 1.298, Accuracy: 44.18%\n",
      "Epoch 15/20 - Loss: 1.299, Accuracy: 44.17%\n",
      "Epoch 16/20 - Loss: 1.298, Accuracy: 44.17%\n",
      "Epoch 17/20 - Loss: 1.298, Accuracy: 44.25%\n",
      "Epoch 18/20 - Loss: 1.298, Accuracy: 44.22%\n",
      "Epoch 19/20 - Loss: 1.298, Accuracy: 44.26%\n",
      "Epoch 20/20 - Loss: 1.297, Accuracy: 44.20%\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(baseline_model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "num_epochs = 20\n",
    "baseline_train_losses = []\n",
    "baseline_train_accuracies = []\n",
    "\n",
    "print(\"Training Baseline Model:\")\n",
    "for epoch in range(num_epochs):\n",
    "    baseline_model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = baseline_model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == batch_y).sum().item()\n",
    "        total += batch_y.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    baseline_train_losses.append(epoch_loss)\n",
    "    baseline_train_accuracies.append(epoch_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.3f}, Accuracy: {epoch_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d7b649-3dd6-4ad1-bb62-9877168f56ab",
   "metadata": {},
   "source": [
    "### Evaluate Baseline Model and Print Sample Predictions\n",
    "\n",
    "We now evaluate the baseline model on the test set. The final test accuracy is printed, and several test samples are displayed (as text), showing the input text, true emotion, and predicted emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a137358-7488-4e83-9a56-aeb3a11a7c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model Final Test Accuracy: 44.36%\n",
      "\n",
      "Sample Test Predictions (Baseline Model):\n",
      "Sample 863: True: anger, Predicted: joy\n",
      "Sample 22056: True: joy, Predicted: joy\n",
      "Sample 25603: True: joy, Predicted: joy\n",
      "Sample 11943: True: sadness, Predicted: anger\n",
      "Sample 8932: True: sadness, Predicted: sadness\n"
     ]
    }
   ],
   "source": [
    "baseline_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        outputs = baseline_model(batch_x)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (preds == batch_y).sum().item()\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_true.extend(batch_y.cpu().numpy())\n",
    "\n",
    "baseline_test_accuracy = 100 * correct / total\n",
    "print(f\"Baseline Model Final Test Accuracy: {baseline_test_accuracy:.2f}%\\n\")\n",
    "\n",
    "# Create a reverse mapping for printing\n",
    "inv_emotion_mapping = {v: k for k, v in emotion_mapping.items()}\n",
    "\n",
    "# Print sample predictions (choose 5 random samples).\n",
    "print(\"Sample Test Predictions (Baseline Model):\")\n",
    "sample_indices = random.sample(range(len(test_dataset)), 5)\n",
    "for idx in sample_indices:\n",
    "    sequence, true_label = test_dataset[idx]\n",
    "    with torch.no_grad():\n",
    "        input_tensor = sequence.unsqueeze(0).to(device)\n",
    "        outputs = baseline_model(input_tensor)\n",
    "        _, pred = torch.max(outputs, 1)\n",
    "    \n",
    "    true_emotion = inv_emotion_mapping[true_label.item()]\n",
    "    pred_emotion = inv_emotion_mapping[pred.item()]\n",
    "    print(f\"Sample {idx}: True: {true_emotion}, Predicted: {pred_emotion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5738add-2e32-4fd1-9ba9-b9ea9399c785",
   "metadata": {},
   "source": [
    "## 2. Improved Model\n",
    "\n",
    "For the improved model, we enhance the architecture by:\n",
    "- Using a bidirectional LSTM,\n",
    "- Adding dropout (in the embedding layer and LSTM),\n",
    "- And adjusting the fully connected layer accordingly.\n",
    "\n",
    "This improved model is expected to generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08aaeb32-afef-4e91-8e72-67cbf86286ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMEmotionClassifierImproved(\n",
      "  (embedding): Embedding(49548, 128)\n",
      "  (dropout_embed): Dropout(p=0.2, inplace=False)\n",
      "  (lstm): LSTM(128, 256, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  (fc): Linear(in_features=512, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LSTMEmotionClassifierImproved(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n",
    "        super(LSTMEmotionClassifierImproved, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.dropout_embed = nn.Dropout(0.2)\n",
    "        # Bidirectional LSTM with dropout\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True, dropout=0.3)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)  # hidden_dim*2 because of bidirection\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        embeds = self.dropout_embed(embeds)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        # Use the output from the last time step (from both directions)\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        logits = self.fc(last_output)\n",
    "        return logits\n",
    "\n",
    "improved_model = LSTMEmotionClassifierImproved(vocab_size, 128, 256, num_classes).to(device)\n",
    "print(improved_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a59f53-6896-4c67-904b-575965be1873",
   "metadata": {},
   "source": [
    "## Train Improved Model\n",
    "\n",
    "The improved model is trained under similar conditions as the baseline model (using Adam optimizer and cross-entropy loss for 10 epochs). Training metrics are printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8937848-5dde-4be2-ab6c-26deb1b30d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Improved Model:\n",
      "Epoch 1/20 - Loss: 1.399, Accuracy: 38.75%\n",
      "Epoch 2/20 - Loss: 1.325, Accuracy: 43.29%\n",
      "Epoch 3/20 - Loss: 1.314, Accuracy: 43.62%\n",
      "Epoch 4/20 - Loss: 1.310, Accuracy: 43.88%\n",
      "Epoch 5/20 - Loss: 1.308, Accuracy: 43.98%\n",
      "Epoch 6/20 - Loss: 1.287, Accuracy: 44.74%\n",
      "Epoch 7/20 - Loss: 0.932, Accuracy: 59.03%\n",
      "Epoch 8/20 - Loss: 0.421, Accuracy: 85.52%\n",
      "Epoch 9/20 - Loss: 0.205, Accuracy: 93.08%\n",
      "Epoch 10/20 - Loss: 0.144, Accuracy: 94.08%\n",
      "Epoch 11/20 - Loss: 0.098, Accuracy: 95.08%\n",
      "Epoch 12/20 - Loss: 0.092, Accuracy: 95.15%\n",
      "Epoch 13/20 - Loss: 0.090, Accuracy: 95.16%\n",
      "Epoch 14/20 - Loss: 0.088, Accuracy: 95.26%\n",
      "Epoch 15/20 - Loss: 0.088, Accuracy: 95.30%\n",
      "Epoch 16/20 - Loss: 0.086, Accuracy: 95.34%\n",
      "Epoch 17/20 - Loss: 0.085, Accuracy: 95.41%\n",
      "Epoch 18/20 - Loss: 0.084, Accuracy: 95.47%\n",
      "Epoch 19/20 - Loss: 0.087, Accuracy: 95.36%\n",
      "Epoch 20/20 - Loss: 0.086, Accuracy: 95.33%\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_improved = optim.Adam(improved_model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "num_epochs_improved = 20\n",
    "improved_train_losses = []\n",
    "improved_train_accuracies = []\n",
    "\n",
    "print(\"Training Improved Model:\")\n",
    "for epoch in range(num_epochs_improved):\n",
    "    improved_model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        optimizer_improved.zero_grad()\n",
    "        outputs = improved_model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer_improved.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == batch_y).sum().item()\n",
    "        total += batch_y.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    improved_train_losses.append(epoch_loss)\n",
    "    improved_train_accuracies.append(epoch_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs_improved} - Loss: {epoch_loss:.3f}, Accuracy: {epoch_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea79b3a2-9483-4104-852d-b2b78b8a1e12",
   "metadata": {},
   "source": [
    "## Evaluate Improved Model and Print Sample Predictions\n",
    "\n",
    "We evaluate the improved model on the test set. Final test accuracy is printed, and a few sample test predictions are displayed in text format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dbe4d625-e55a-4b71-a3df-44abd3713bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved Model Final Test Accuracy: 95.41%\n",
      "\n",
      "Sample Test Predictions (Improved Model):\n",
      "Sample 13195: True: sadness, Predicted: sadness\n",
      "Sample 3132: True: joy, Predicted: joy\n",
      "Sample 25032: True: fear, Predicted: fear\n",
      "Sample 21747: True: sadness, Predicted: sadness\n",
      "Sample 22213: True: anger, Predicted: anger\n"
     ]
    }
   ],
   "source": [
    "improved_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        outputs = improved_model(batch_x)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (preds == batch_y).sum().item()\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_true.extend(batch_y.cpu().numpy())\n",
    "\n",
    "improved_test_accuracy = 100 * correct / total\n",
    "print(f\"Improved Model Final Test Accuracy: {improved_test_accuracy:.2f}%\\n\")\n",
    "\n",
    "# Print sample predictions \n",
    "print(\"Sample Test Predictions (Improved Model):\")\n",
    "sample_indices = random.sample(range(len(test_dataset)), 5)\n",
    "for idx in sample_indices:\n",
    "    sequence, true_label = test_dataset[idx]\n",
    "    with torch.no_grad():\n",
    "        input_tensor = sequence.unsqueeze(0).to(device)\n",
    "        outputs = improved_model(input_tensor)\n",
    "        _, pred = torch.max(outputs, 1)\n",
    "    \n",
    "    true_emotion = inv_emotion_mapping[true_label.item()]\n",
    "    pred_emotion = inv_emotion_mapping[pred.item()]\n",
    "    print(f\"Sample {idx}: True: {true_emotion}, Predicted: {pred_emotion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75720f45-349a-42d5-b324-ac4a1ba9da9e",
   "metadata": {},
   "source": [
    "# Discussion and Findings\n",
    "\n",
    "The emotion detection system was implemented using two custom deep learning models built entirely from scratch—one serving as the baseline and the other as an improved version.\n",
    "\n",
    "**Baseline Model:**  \n",
    "- The baseline LSTM-based classifier achieved a final test accuracy of **44.36%**.  \n",
    "- This model, which employs a simple unidirectional LSTM without additional regularization or context-enhancing mechanisms, struggled to capture the complex emotional nuances within the text. The low accuracy indicates that a basic architecture is insufficient for robust emotion classification.\n",
    "\n",
    "**Improved Model:**  \n",
    "- The improved model, which incorporates a bidirectional LSTM along with dropout regularization, achieved a final test accuracy of **95.61%**.  \n",
    "- By processing text in both forward and backward directions, the model effectively captures a more comprehensive context. The application of dropout further helps mitigate overfitting, leading to robust performance even on unseen data.\n",
    "\n",
    " | Model | Accuracy |\n",
    "|------------------|------------|\n",
    "| **Base** | **44.36%** |\n",
    "| **Improved** | **95.61%** |\n",
    "\n",
    "\n",
    "**Summary of Findings:**  \n",
    "- The stark contrast between the baseline model (44.36%) and the improved model (95.61%) underscores the crucial role of leveraging bidirectional context and regularization in emotion classification tasks.\n",
    "- While the baseline model's simplicity leaves it unable to model the intricate dependencies present in emotional language, the enhanced complexity of the improved model significantly boosts performance.\n",
    "- These results highlight that for emotion classification, advanced architectural designs can dramatically improve a model's ability to capture the subtleties of natural language.\n",
    "\n",
    "Overall, the findings demonstrate that a carefully tailored deep learning approach—built without pre-trained components—can achieve high accuracy in emotion detection, provided that the model is equipped to process context comprehensively and is well-regularized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae88846d-1fce-4285-ad07-dca701cb5ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
